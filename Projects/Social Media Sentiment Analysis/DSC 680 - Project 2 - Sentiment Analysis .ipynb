{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1901b0e3",
   "metadata": {},
   "source": [
    "1. Preprocessing the Data\n",
    "Before training your model, you need to preprocess your text data to convert it into a format that the machine learning algorithm can work with.\n",
    "\n",
    "Tokenization: Split the text into words or tokens.\n",
    "Removing Noise: Filter out unnecessary data, like special characters, numbers, and punctuation.\n",
    "Stopwords Removal: Remove common words that might not contribute much to the sentiment analysis.\n",
    "Stemming or Lemmatization: Reduce words to their base or root form.\n",
    "Python's nltk library can be very helpful for these tasks.\n",
    "\n",
    "2. Vectorization\n",
    "After preprocessing, convert the text data into numerical features. Common techniques include:\n",
    "\n",
    "Bag of Words: Count the frequency of each word in each document.\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency): Weigh the frequencies by how unique they are to the document in the context of the entire dataset.\n",
    "You can use CountVectorizer or TfidfVectorizer from Python's sklearn.feature_extraction.text for this step.\n",
    "\n",
    "3. Training the Naïve Bayes Classifier\n",
    "With the data preprocessed and vectorized, you can now train the Naïve Bayes classifier.\n",
    "\n",
    "Choosing the Model: Depending on the nature of your feature vectors, choose between different types of Naïve Bayes classifiers available in sklearn.naive_bayes. For instance, MultinomialNB works well with counts (like from Bag of Words), while GaussianNB is used when features are continuous.\n",
    "Training the Model: Use the training dataset to train the model.\n",
    "4. Model Evaluation\n",
    "Evaluate the model using your validation dataset to see how well it performs.\n",
    "\n",
    "Accuracy: Measure how often the classifier correctly predicts the sentiment.\n",
    "Confusion Matrix: Useful to see the true positives, true negatives, false positives, and false negatives.\n",
    "Precision, Recall, and F1-Score: These metrics will help you understand the balance between precision and recall, giving you a better picture of overall performance.\n",
    "5. Fine-tuning and Improvements\n",
    "Depending on the initial results, you may need to adjust your preprocessing, tweak the model, or choose different parameters.\n",
    "\n",
    "Hyperparameter Tuning: Adjust parameters like alpha in Naïve Bayes.\n",
    "Cross-Validation: Use cross-validation to ensure your model's performance is stable across different subsets of your data.\n",
    "6. Implementation\n",
    "Implement the model to make predictions on new tweets to see the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d30bfc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\matt7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\matt7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\matt7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29cae376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('twitter_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37594a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet ID      Entity  Sentiment  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "\n",
       "                                       Tweet Content  \n",
       "0  im getting on borderlands and i will murder yo...  \n",
       "1  I am coming to the borders and I will kill you...  \n",
       "2  im getting on borderlands and i will kill you ...  \n",
       "3  im coming on borderlands and i will murder you...  \n",
       "4  im getting on borderlands 2 and i will murder ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e3ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c7b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Check nan values\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    # Tokenization & lowercasing\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Removing punctuation & nums\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join words \n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Apply\n",
    "data['Processed Tweet Content'] = data['Tweet Content'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35820b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet Content</th>\n",
       "      <th>Processed Tweet Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>im getting borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>coming border kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>im getting borderland kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>im coming borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>im getting borderland murder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet ID      Entity  Sentiment  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "\n",
       "                                       Tweet Content  \\\n",
       "0  im getting on borderlands and i will murder yo...   \n",
       "1  I am coming to the borders and I will kill you...   \n",
       "2  im getting on borderlands and i will kill you ...   \n",
       "3  im coming on borderlands and i will murder you...   \n",
       "4  im getting on borderlands 2 and i will murder ...   \n",
       "\n",
       "        Processed Tweet Content  \n",
       "0  im getting borderland murder  \n",
       "1            coming border kill  \n",
       "2    im getting borderland kill  \n",
       "3   im coming borderland murder  \n",
       "4  im getting borderland murder  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ad7f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc227548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer & transform \n",
    "tfidf_matrix = vectorizer.fit_transform(data['Processed Tweet Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0e73399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaa' 'aaaaaaaaaaaa' 'aaaaaaaaaaaaa'\n",
      " 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'\n",
      " 'aaaaaaaaaaages' 'aaaaaaaaaages' 'aaaaaasee' 'aaaahhh' 'aadii' 'aadil'\n",
      " 'aajtak' 'aall' 'aamaavpjyc' 'aarogya' 'aaron' 'aaroncarter'\n",
      " 'aarongreenberg' 'aat' 'aatmanirbhar' 'aatmanirvar' 'ab' 'aback' 'abah'\n",
      " 'abandon']\n"
     ]
    }
   ],
   "source": [
    "# Get feature names \n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Preview feature names\n",
    "print(feature_names[:25]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc19bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training the Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b14f3536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your features (TF-IDF vectors) & labels (Sentiments)\n",
    "X = tfidf_matrix\n",
    "y = data['Sentiment']\n",
    "\n",
    "# Split data training and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60f20ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Multinomial Naïve Bayes classifier\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Train \n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed131f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7036218785566044\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.93      0.41      0.57      2592\n",
      "    Negative       0.64      0.88      0.74      4519\n",
      "     Neutral       0.78      0.59      0.67      3596\n",
      "    Positive       0.69      0.79      0.74      4230\n",
      "\n",
      "    accuracy                           0.70     14937\n",
      "   macro avg       0.76      0.67      0.68     14937\n",
      "weighted avg       0.74      0.70      0.69     14937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiments test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe93f2",
   "metadata": {},
   "source": [
    "Analyzing Model Evaluation Results\n",
    "Accuracy: The model correctly predicted the sentiment of tweets about 70.36% of the time across all classes, which is fair for a preliminary model but suggests there is room for improvement.\n",
    "\n",
    "Precision and Recall:\n",
    "\n",
    "Irrelevant: High precision (0.93) but low recall (0.41) suggests that while most of the predictions for this class are correct, the model is missing a significant number of 'Irrelevant' tweets.\n",
    "Negative: Decent precision (0.64) with high recall (0.88). The model is capable of identifying most 'Negative' tweets, though it also includes a moderate number of false positives.\n",
    "Neutral: Both precision (0.78) and recall (0.59) are moderate, indicating a balanced but not highly accurate performance on this class.\n",
    "Positive: Moderate precision (0.69) and good recall (0.79). The model is relatively successful in identifying 'Positive' tweets but with some false positives.\n",
    "F1-Score: Gives a balance between precision and recall. We notice that 'Negative' and 'Positive' sentiments have the highest F1 scores, which are the classes better handled by the model compared to 'Irrelevant' and 'Neutral'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6d5c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Fine-tuning and Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d92693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust parameters\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, ngram_range=(1, 2))\n",
    "\n",
    "# Fit & transform \n",
    "tfidf_matrix = vectorizer.fit_transform(data['Processed Tweet Content'])\n",
    "\n",
    "# Split data\n",
    "X = tfidf_matrix\n",
    "y = data['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fcebda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.911093258351744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.96      0.86      0.91      2592\n",
      "    Negative       0.87      0.95      0.91      4519\n",
      "     Neutral       0.93      0.89      0.91      3596\n",
      "    Positive       0.91      0.91      0.91      4230\n",
      "\n",
      "    accuracy                           0.91     14937\n",
      "   macro avg       0.92      0.91      0.91     14937\n",
      "weighted avg       0.91      0.91      0.91     14937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Multinomial Naïve Bayes classifier\n",
    "model = MultinomialNB(alpha=0.1) \n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict sentiments test \n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d6e45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05afd7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 Preprocessing\n",
    "\n",
    "# Load validation dataset\n",
    "validation_data = pd.read_csv('twitter_validation.csv')\n",
    "\n",
    "validation_data['Processed Tweet Content'] = validation_data['Tweet Content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a0a7289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Vectorize the Preprocessed Text\n",
    "X_validation = vectorizer.transform(validation_data['Processed Tweet Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ab8ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Make Predictions\n",
    "y_validation_pred = model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49f160b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.99      0.98      0.99       172\n",
      "    Negative       0.97      0.98      0.98       266\n",
      "     Neutral       0.99      0.97      0.98       285\n",
      "    Positive       0.97      0.99      0.98       277\n",
      "\n",
      "    accuracy                           0.98      1000\n",
      "   macro avg       0.98      0.98      0.98      1000\n",
      "weighted avg       0.98      0.98      0.98      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Evaluate the Predictions\n",
    "\n",
    "# True\n",
    "y_validation_true = validation_data['Sentiment']\n",
    "\n",
    "# Accuracy\n",
    "accuracy_validation = accuracy_score(y_validation_true, y_validation_pred)\n",
    "print(f\"Validation Accuracy: {accuracy_validation}\")\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_validation_true, y_validation_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73053d",
   "metadata": {},
   "source": [
    "These results look fantastic! Your model achieved a high validation accuracy of approximately 98.1%, indicating its strong performance on the validation dataset. Here's a breakdown of the evaluation metrics:\n",
    "\n",
    "Precision, Recall, and F1-Score: Across all sentiment classes ('Irrelevant', 'Negative', 'Neutral', and 'Positive'), precision, recall, and F1-score values are consistently high, ranging from approximately 97% to 99%. This indicates that the model is performing exceptionally well in correctly identifying tweets belonging to each sentiment category, with balanced precision and recall.\n",
    "\n",
    "Macro and Weighted Averages: Both macro and weighted averages for precision, recall, and F1-score are approximately 0.98, which confirms the overall effectiveness of the model across all sentiment classes.\n",
    "\n",
    "Conclusion\n",
    "With a validation accuracy of around 98.1%, your model demonstrates remarkable performance in accurately predicting the sentiment of tweets in the validation dataset. This high accuracy indicates that your model generalizes well to unseen data, making it suitable for real-world applications.\n",
    "\n",
    "If you're satisfied with these results, you can consider this step successful. However, it's always a good practice to continue monitoring and evaluating the model's performance over time, especially as new data becomes available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8795497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
